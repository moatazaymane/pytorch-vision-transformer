batch_size = 150
n_epochs = 30
patch_size = 4  # paper 16/ 32
D = 512  # paper 1024
Dmlp = 1024  # paper 2048
lr_gap = 2e-5
lr_cls = 8e-4
h, k = 8, 8  # splitting the q k v representation into h q,k,v # k in original paper
L = 12  # number of encoder blocks
dropout = 0.1
imgsize = 32
num_classes = 10
n_channels = 3
model_path_l16 = "/content/pytorch-vision-transformer/drive/MyDrive/models/vit_l16"
load_pretrained = False
train_data_path = "/content/pytorch-vision-transformer/drive/MyDrive/models/train_dataset.pt"
targets_path = "/content/pytorch-vision-transformer/drive/MyDrive/models/targets.pt"
loss_path_l16 = "/content/pytorch-vision-transformer/drive/MyDrive/models/loss_l16.json"
accuracy_path_l16 = "/content/pytorch-vision-transformer/drive/MyDrive/models/accuracy_l16.json"
